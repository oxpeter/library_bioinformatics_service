{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Pandas : Joins, slices, and subsetting\n",
    "#### A short workshop run by the Library Bioinformatics Service\n",
    "Based on the Data Carpentry curriculum for [Data Visualization in Python](http://www.datacarpentry.org/python-ecology-lesson/)\n",
    "( Â© Data Carpentry under [Creative Commons Attribution\n",
    "license](https://creativecommons.org/licenses/by/4.0/) )\n",
    "\n",
    "---\n",
    "\n",
    "### Our Data\n",
    "\n",
    "As for the last lesson, we will be using NCD Risk Factor Collaboration (NDC-RisC)  data, from `Worldwide trends in body-mass index, underweight, overweight, and obesity from 1975 to 2016: a pooled analysis of 2416 population-based measurement studies in 128.9 million children, adolescents, and adults.` [Lancet 2017, published online 11 October 2017](http://www.ncdrisc.org/index.html)\n",
    "\n",
    "We will be using the adult BMI dataset [country-specific data](http://www.ncdrisc.org/downloads/bmi/NCD_RisC_Lancet_2017_BMI_age_standardised_country.csv) and the height at age 18 [country specific data](http://www.ncdrisc.org/downloads/height/NCD_RisC_eLife_2016_height_age18_countries.csv).\n",
    "\n",
    "The downloaded filename is `NCD_RisC_Lancet_2017_BMI_age_standardised_country.csv`, but we have saved a simplified version of the file here as `NCD_RisC_bmi.csv`.\n",
    "\n",
    "The table of arable land (hectares per person) is taken from the [World Bank Databank](https://data.worldbank.org/indicator/AG.LND.ARBL.HA.PC?view=chart)\n",
    "\n",
    "The table of life expectancy is taken from [our world in data](https://ourworldindata.org/life-expectancy)\n",
    "\n",
    "---\n",
    "\n",
    "In many \"real world\" situations, the data that we want to use come in multiple\n",
    "files. We often need to combine these files into a single DataFrame to analyze\n",
    "the data. The pandas package provides [various methods for combining\n",
    "DataFrames](http://pandas.pydata.org/pandas-docs/stable/merging.html) including\n",
    "`merge` and `concat`.\n",
    "\n",
    "To work through the examples below, we first need to load the species and\n",
    "surveys files into pandas DataFrames. In iPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library:\n",
    "import pandas as pd\n",
    "\n",
    "# set the plots to appear 'inline' in the notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmi = pd.read_csv(\"data/NCD_RisC_bmi.csv\")\n",
    "df_bmi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_height = pd.read_csv(\"data/NCD_RisC_height.csv\")\n",
    "df_height.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the `read_csv` method we used can take some additional options. Many functions in python have a set of options that\n",
    "can be set by the user if needed.\n",
    "[More about all of the read_csv options here.](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating DataFrames\n",
    "\n",
    "We can use the `concat` function in Pandas to append either columns or rows from\n",
    "one DataFrame to another.  Let's grab two subsets of our data to see how this\n",
    "works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in first 10 lines of bmi table\n",
    "bmi_sub_first10 = df_bmi.head(10)\n",
    "\n",
    "# Grab the last 10 rows\n",
    "bmi_sub_last10 = df_bmi.tail(10)\n",
    "\n",
    "# Reset the index values so the second dataframe appends properly\n",
    "bmi_sub_last10=bmi_sub_last10.reset_index(drop=True)\n",
    "# drop=True option avoids adding new index column with old index values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we concatenate DataFrames, we need to specify the axis. `axis=0` tells\n",
    "Pandas to stack the second DataFrame under the first one. It will automatically\n",
    "detect whether the column names are the same and will stack accordingly.\n",
    "`axis=1` will stack the columns in the second DataFrame to the RIGHT of the\n",
    "first DataFrame. To stack the data vertically, we need to make sure we have the\n",
    "same columns and associated column format in both datasets. When we stack\n",
    "horizonally, we want to make sure what we are doing makes sense (ie the data are\n",
    "related in some way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the DataFrames on top of each other\n",
    "vertical_stack = pd.concat([bmi_sub_first10, bmi_sub_last10], axis=0)\n",
    "\n",
    "# Place the DataFrames side by side\n",
    "horizontal_stack = pd.concat([bmi_sub_first10, bmi_sub_last10], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Index Values and Concat\n",
    "Have a look at the `vertical_stack` dataframe? Notice anything unusual?\n",
    "The row indexes for the two data frames `bmi_sub_first10` and `bmi_sub_last10`\n",
    "have been repeated. We can reindex the new dataframe using the `reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to view your vertical_stack dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to view your horizontal_stack dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to re-index the vertical_stack dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Out Data to CSV\n",
    "\n",
    "We can use the `to_csv` command to do export a DataFrame in CSV format. Note that the code\n",
    "below will by default save the data into the current working directory. We can\n",
    "save it to a different folder by adding the foldername and a slash to the file\n",
    "`vertical_stack.to_csv('foldername/out.csv')`. We use the 'index=False' so that\n",
    "pandas doesn't include the index number for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to CSV\n",
    "vertical_stack.to_csv('data/vertical_stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your working directory to make sure the CSV wrote out properly, and\n",
    "that you can open it! If you want, try to bring it back into python to make sure\n",
    "it imports properly.\n",
    "\n",
    "Make sure that any directory you are specifying has already been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For kicks read our output back into python and make sure all looks good\n",
    "new_output = pd.read_csv('data/vertical_stack.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Challenge - Combine Data\n",
    ">\n",
    "> In the data folder, there are two survey data files: `NCD_RisC_height_1896.csv` and\n",
    "> `NCD_RisC_height_1996.csv`. Read the data into python and combine the files to make one\n",
    "> new data frame. Create a plot of average plot weight by year grouped by sex.\n",
    "> Export your results as a CSV and make sure it reads back into python properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining DataFrames\n",
    "\n",
    "When we concatenated our DataFrames we simply added them to each other -\n",
    "stacking them either vertically or side by side. Another way to combine\n",
    "DataFrames is to use columns in each dataset that contain common values (a\n",
    "common unique id). Combining DataFrames using a common field is called\n",
    "\"joining\". The columns containing the common values are called \"join key(s)\".\n",
    "Joining DataFrames in this way is often useful when one DataFrame is a \"lookup\n",
    "table\" containing additional data that we want to include in the other.\n",
    "\n",
    "NOTE: This process of joining tables is similar to what we do with tables in an\n",
    "SQL database.\n",
    "\n",
    "For example, we might create a lookup table for our country data. This table \n",
    "could contain information such as economic indicators, population, area for each\n",
    "of the countries. The\n",
    "country code would be unique for each line.  Rather than adding multiple extra columns\n",
    "to each of the 16,800 lines of the bmi data table, we\n",
    "can maintain the shorter table with the country information. When we want to\n",
    "access that information, we can create a query that joins the additional columns\n",
    "of information to the Survey data.\n",
    "\n",
    "Storing data in this way has many benefits including:\n",
    "\n",
    "1. It ensures consistency in the spelling of species attributes (genus, species\n",
    "   and taxa) given each species is only entered once. Imagine the possibilities\n",
    "   for spelling errors when entering the genus and species thousands of times!\n",
    "2. It also makes it easy for us to make changes to the species information once\n",
    "   without having to find each instance of it in the larger survey data.\n",
    "3. It optimizes the size of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Two DataFrames\n",
    "\n",
    "To better understand joins, let's grab the first 10 lines of our data as a\n",
    "subset to work with. We'll use the `.head` method to do this. We'll also read\n",
    "in a subset of the hieght table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in first 10 lines of bmi table\n",
    "bmi_sub = df_bmi.head(10)\n",
    "\n",
    "# Import a small subset of the species data designed for this part of the lesson.\n",
    "# It is stored in the data folder.\n",
    "df_arable = pd.read_csv('data/arable_land.csv')\n",
    "arable_sub = df_arable.head(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `df_arable` is the lookup table containing country code, country, and\n",
    "hectares of arable land per person, that we want to join with the data in `survey_sub` to produce a new\n",
    "DataFrame that contains all of the columns from both `df_bmi` *and*\n",
    "`df_arable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying join keys\n",
    "\n",
    "To identify appropriate join keys we first need to know which field(s) are\n",
    "shared between the files (DataFrames). We might inspect both DataFrames to\n",
    "identify these columns. If we are lucky, both DataFrames will have columns with\n",
    "the same name that also contain the same data. If we are less lucky, we need to\n",
    "identify a (differently-named) column in each DataFrame that contains the same\n",
    "information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arable_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_sub.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the join key is the column containing the three-letter country\n",
    "identifier, which is called `iso`.\n",
    "\n",
    "Now that we know the fields with the common country ID attributes in each\n",
    "DataFrame, we are almost ready to join our data. However, since there are\n",
    "[different types of joins](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/), we\n",
    "also need to decide which type of join makes sense for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner joins\n",
    "\n",
    "The most common type of join is called an _inner join_. An inner join combines\n",
    "two DataFrames based on a join key and returns a new DataFrame that contains\n",
    "**only** those rows that have matching values in *both* of the original\n",
    "DataFrames.\n",
    "\n",
    "Inner joins yield a DataFrame that contains only rows where the value being\n",
    "joined exists in BOTH tables. An example of an inner join, adapted from [this\n",
    "page](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) is below:\n",
    "\n",
    "![Inner join -- courtesy of codinghorror.com](http://blog.codinghorror.com/content/images/uploads/2007/10/6a0120a85dcdae970b012877702708970c-pi.png)\n",
    "\n",
    "The pandas function for performing joins is called `merge` and an Inner join is\n",
    "the default option:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inner = pd.merge(left=bmi_sub,\n",
    "                        right=arable_sub, \n",
    "                        left_on='iso', \n",
    "                        right_on='iso')\n",
    "# In this case `species_id` is the only column name in  both dataframes, so if we skippd `left_on`\n",
    "# And `right_on` arguments we would still get the same result\n",
    "\n",
    "# What's the size of the output data?\n",
    "merged_inner.shape\n",
    "merged_inner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of an inner join of `bmi_sub` and `arable_sub` is a new DataFrame\n",
    "that contains the combined set of columns from `bmi_sub` and `arable_sub`. It\n",
    "*only* contains rows that have three-letter country codes that are the same in\n",
    "both the `bmi_sub` and `arable_sub` DataFrames. In other words, if a row in\n",
    "`bmi_sub` has a value of `iso` that does *not* appear in the `iso`\n",
    "column of `arable_sub`, it will not be included in the DataFrame returned by an\n",
    "inner join.  Similarly, if a row in `arable_sub` has a value of `iso`\n",
    "that does *not* appear in the `iso` column of `bmi_sub`, that row will not\n",
    "be included in the DataFrame returned by an inner join.\n",
    "\n",
    "The two DataFrames that we want to join are passed to the `merge` function using\n",
    "the `left` and `right` argument. The `left_on='iso'` argument tells `merge`\n",
    "to use the `iso` column as the join key from `bmi_sub` (the `left`\n",
    "DataFrame). Similarly , the `right_on='iso'` argument tells `merge` to\n",
    "use the `iso` column as the join key from `arable_sub` (the `right`\n",
    "DataFrame). For inner joins, the order of the `left` and `right` arguments does\n",
    "not matter.\n",
    "\n",
    "The resulting `merged_inner` DataFrame contains all of the columns from `bmi_sub`\n",
    "(iso, year, bmi, etc.) as well as all the columns from `arable_sub`\n",
    "(iso, country, and arable land per person).\n",
    "\n",
    "Notice that `merged_inner` has fewer rows than `arable_sub`. This is an\n",
    "indication that there were rows in `bmi_sub` with value(s) for `iso` that\n",
    "do not exist as value(s) for `iso` in `arable_sub`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left joins\n",
    "\n",
    "What if we want to add information from `arable_sub` to `bmi_sub` without\n",
    "losing any of the information from `bmi_sub`? In this case, we use a different\n",
    "type of join called a \"left outer join\", or a \"left join\".\n",
    "\n",
    "Like an inner join, a left join uses join keys to combine two DataFrames. Unlike\n",
    "an inner join, a left join will return *all* of the rows from the `left`\n",
    "DataFrame, even those rows whose join key(s) do not have values in the `right`\n",
    "DataFrame.  Rows in the `left` DataFrame that are missing values for the join\n",
    "key(s) in the `right` DataFrame will simply have null (i.e., NaN or None) values\n",
    "for those columns in the resulting joined DataFrame.\n",
    "\n",
    "Note: a left join will still discard rows from the `right` DataFrame that do not\n",
    "have values for the join key(s) in the `left` DataFrame.\n",
    "\n",
    "![Left Join](https://www.dataquest.io/blog/content/images/2017/12/left-join.png)\n",
    "\n",
    "A left join is performed in pandas by calling the same `merge` function used for\n",
    "inner join, but using the `how='left'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left = pd.merge(left=bmi_sub,\n",
    "                       right=arable_sub, \n",
    "                       how='left', \n",
    "                       left_on='iso', \n",
    "                       right_on='iso')\n",
    "\n",
    "merged_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result DataFrame from a left join (`merged_left`) looks very much like the\n",
    "result DataFrame from an inner join (`merged_inner`) in terms of the columns it\n",
    "contains. However, unlike `merged_inner`, `merged_left` contains the **same\n",
    "number of rows** as the original `bmi_sub` DataFrame. When we inspect\n",
    "`merged_left`, we find there are rows where the information that should have\n",
    "come from `arable_sub` (i.e., `country_name`, and `arable_land_ha_pp`) is\n",
    "missing (they contain NaN values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left[ pd.isnull(merged_left.country_name) ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rows are the ones where the value of `iso` from `bmi_sub` (in this\n",
    "case, `SVK`, `JPN`, `MKD`, etc) does not occur in `arable_sub`.\n",
    "\n",
    "\n",
    "## Other join types\n",
    "\n",
    "The pandas `merge` function supports two other join types:\n",
    "\n",
    "* Right (outer) join: Invoked by passing `how='right'` as an argument. Similar\n",
    "  to a left join, except *all* rows from the `right` DataFrame are kept, while\n",
    "  rows from the `left` DataFrame without matching join key(s) values are\n",
    "  discarded.\n",
    "* Full (outer) join: Invoked by passing `how='outer'` as an argument. This join\n",
    "  type returns the all pairwise combinations of rows from both DataFrames; i.e.,\n",
    "  the result DataFrame will `NaN` where data is missing in one of the dataframes. This join type is\n",
    "  very rarely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Challenges\n",
    "\n",
    "> ## Challenge - Distributions\n",
    "> Create a new DataFrame by joining the contents of the `NCD_RisC_bmi.csv` and\n",
    "> `NCD_RisC_height.csv` tables. Then calculate and plot the distribution of:\n",
    ">\n",
    "> 1. bmi by country\n",
    "> 2. bmi by sex by country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Challenge - Weight by geographical region\n",
    ">\n",
    "> 1. In the data folder, there is a `CSV` that contains information about the\n",
    ">    countries assigned to different geographical regions. Use that data to \n",
    "> summarize the number of countries by region.\n",
    "> 2. Calculate an expected mean weight of each region, using the bmi and height data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = pd.read_csv('data/world_regions.csv')\n",
    "df_bmi = pd.read_csv('data/NCD_RisC_bmi.csv')\n",
    "df_height = pd.read_csv('data/NCD_RisC_height.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing Subsets of Rows and Columns in Python\n",
    "\n",
    "We can select specific ranges of our data in both the row and column directions\n",
    "using either label or integer-based indexing.\n",
    "\n",
    "- `loc` is primarily *label* based indexing. *Integers* may be used but\n",
    "  they are interpreted as a *label*.\n",
    "- `iloc` is primarily *integer* based indexing\n",
    "\n",
    "To select a subset of rows **and** columns from our DataFrame, we can use the\n",
    "`iloc` method. For example, we can select month, day and year (columns 2, 3\n",
    "and 4 if we start counting at 1), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc[row slicing, column slicing]\n",
    "df_regions.iloc[0:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives the **output**\n",
    "\n",
    "```\n",
    "\talpha-3\tregion\tsub-region\n",
    "0\tAFG\tAsia\tSouthern Asia\n",
    "1\tALA\tEurope\tNorthern Europe\n",
    "2\tALB\tEurope\tSouthern Europe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we asked for a slice from 0:3. This yielded 3 rows of data. When you\n",
    "ask for 0:3, you are actually telling Python to start at index 0 and select rows\n",
    "0, 1, 2 **up to but not including 3**.\n",
    "\n",
    "Let's explore some other ways to index and select subsets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns for rows of index values 0 and 10\n",
    "df_regions.loc[[0, 10], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this do?\n",
    "df_regions.loc[0, ['name', 'alpha-3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when you type the code below?\n",
    "df_regions.loc[[0, 10, 149], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Labels must be found in the DataFrame or you will get a `KeyError`.\n",
    "\n",
    "Indexing by labels `loc` differs from indexing by integers `iloc`.\n",
    "With `loc`, the both start bound and the stop bound are **inclusive**. When using\n",
    "`loc`, integers *can* be used, but the integers refer to the\n",
    "index label and not the position. For example, using `loc` and select 1:4\n",
    "will get a different result than using `iloc` to select rows 1:4.\n",
    "\n",
    "We can also select a specific data value using a row and\n",
    "column location within the DataFrame and `iloc` indexing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Syntax for iloc indexing to finding a specific data element\n",
    "dat.iloc[row, column]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions.iloc[23, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Python indexing begins at 0. So, the index location [23, 3]\n",
    "selects the element that is 24 rows down and 4 columns over in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Challenge - Range\n",
    ">\n",
    "> 1. What happens when you execute:\n",
    ">\n",
    ">    - `df_bmi[0:1]`\n",
    ">    - `df_bmi[:4]`\n",
    ">    - `df_bmi[:-1]`\n",
    ">\n",
    "> 2. What happens when you call:\n",
    ">\n",
    ">    - `df_bmi.iloc[0:4, 1:4]`\n",
    ">    - `df_bmi.loc[0:4, 1:4]`\n",
    ">\n",
    "> - How are the two commands different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting Data using Criteria\n",
    "\n",
    "We can also select a subset of our data using criteria. For example, we can\n",
    "select all rows that have a year value of 2002:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions[df_regions.region == 'Oceania']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can select all rows that do not contain the year 2002:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions[df_regions.region != 'Oceania']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define sets of criteria too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions[(df_regions.region == 'Asia') | (df_regions.region == 'Europe')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Syntax Cheat Sheet\n",
    "\n",
    "Use can use the syntax below when querying data by criteria from a DataFrame.\n",
    "Experiment with selecting various subsets of the \"surveys\" data.\n",
    "\n",
    "* Equals: `==`\n",
    "* Not equals: `!=`\n",
    "* Greater than, less than: `>` or `<`\n",
    "* Greater than or equal to `>=`\n",
    "* Less than or equal to `<=`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Challenge - Queries\n",
    ">\n",
    "> 1. Select a subset of rows in the `df_bmi` DataFrame that contain data from\n",
    ">   the year 1999 and that contain mean bmi values less than or equal to 18. How\n",
    ">   many rows did you end up with?\n",
    ">\n",
    "> 2. You can use the `isin` command in Python to query a DataFrame based upon a\n",
    ">   list of values as follows:\n",
    ">\n",
    ">    ```python\n",
    ">    df_bmi[df_bmi['iso'].isin([listGoesHere])]\n",
    ">    ```\n",
    ">\n",
    ">   Use the `isin` function to find all bmi records from a list of countries\n",
    ">   in the \"df_bmi\" DataFrame. How many records contain these values?\n",
    ">\n",
    "> 3. Experiment with other queries. Create a query that finds all rows with a\n",
    ">   bmi between 20 and 23.\n",
    ">\n",
    "> 4. The `~` symbol in Python can be used to return the OPPOSITE of the\n",
    ">   selection that you specify in Python. It is equivalent to **is not in**.\n",
    ">   Write a query that selects all rows with year NOT equal to 1996 or 2004 in\n",
    ">   the \"df_bmi\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
